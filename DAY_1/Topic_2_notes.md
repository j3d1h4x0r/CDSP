# ğŸ”§ Topic 2: Extract, Transform, and Load (ETL) Data

This topic introduces the core data engineering workflow used in every data science project: **ETL**.

---

## ğŸ”„ ETL Overview

**ETL** stands for:

- **Extract**: Pulling data from various sources
- **Transform**: Cleaning, reshaping, and preparing data
- **Load**: Storing data for modeling or analysis

ETL ensures that your models and analytics work with clean, structured, and usable data.

---

## ğŸ’¾ 1. Extracting Data

### ğŸ“Š Types of Data

- **Structured**: Tables, databases (SQL, CSV, Excel)
- **Unstructured**: Images, audio, PDFs, social media posts
- **Semi-structured**: JSON, XML

### ğŸ¤ Common Data Sources

- CSV/Excel files
- SQL/NoSQL databases
- APIs (e.g., RESTful APIs)
- Cloud storage (S3, Azure Blob, Google Cloud)
- Web scraping (HTML parsing)

### ğŸ§‘â€ğŸ’¼ First-Party vs Third-Party Data

- **First-party**: Owned by the organization
- **Second-party**: Shared partner data
- **Third-party**: External datasets (e.g., Kaggle, AWS, public APIs)

### ğŸ›ï¸ Open Datasets

**Popular sources:**

- [Kaggle Datasets](https://www.kaggle.com/datasets)
- [UCI ML Repository](https://archive.ics.uci.edu/ml)
- [Data.gov](https://www.data.gov)
- [OpenML](https://www.openml.org)

### ğŸ” Data Access Practices

- Use read-only permissions
- Secure API keys and tokens
- Minimize extracted columns
- Anonymize PII early

### âš™ï¸ Practical Tip

Use tools like `pandas.read_csv()`, `requests.get()`, and `SQLAlchemy` to extract and preview datasets in Python.

### ğŸ”¹ Use Case

A telecom company pulls user transaction records from PostgreSQL and merges them with marketing data in CSV format.

---

## ğŸ’¡ 2. Transforming Data

### ğŸƒ Data Cleaning Tasks

- Remove duplicates
- Handle missing values
- Fix data types
- Format dates
- Normalize units (e.g., lbs â†’ kg)

### ğŸ—ï¸ Data Structuring

- Merge/join multiple tables
- Pivot/unpivot data
- Aggregate rows (e.g., transaction totals per user)

### âš–ï¸ Feature Types

- **Quantitative**: Numerical (e.g., income)
- **Qualitative**: Categorical (e.g., gender)
- **Ordinal**: Ranked categories (e.g., education level)

### â†”ï¸ Continuous vs Discrete

- **Continuous**: Infinite values (e.g., height)
- **Discrete**: Countable (e.g., number of purchases)

### ğŸ§° Common Python Libraries

- `pandas`: For all tabular data operations
- `numpy`: Numeric transformations
- `datetime`: Time-related parsing

### ğŸ”¹ Use Case

A bank has transaction data in seconds. To analyze call durations, durations are aggregated into minutes and then grouped by customer.

### âœ… Governance Considerations

- Document all transformations
- Ensure data security during transformation
- Anonymize before joining or exporting

---

## ğŸ› ï¸ 3. Loading Data

### ğŸ“‚ Load Destinations

- SQL databases
- Data warehouses (e.g., Snowflake, BigQuery)
- Dashboards (e.g., Tableau, Power BI)
- Files (CSV, JSON, Parquet)

### â†”ï¸ Merging & Joining

- **Inner Join**: Only matching rows
- **Left Join**: All from left, matched from right
- **Right Join**: All from right, matched from left
- **Outer Join**: All from both, matched or not

### ğŸ’¡ Practical Tips

- Avoid full refresh unless necessary
- Use incremental loads for large datasets
- Track data versions

### ğŸ”¹ Use Case

After cleaning and combining transaction and user data, the final DataFrame is exported to a PostgreSQL database for dashboard reporting.

---

## ğŸ“‹ ETL Checklist

- [x] Locate all required data sources
- [x] Validate file integrity and freshness
- [x] Use appropriate tools to extract and preview
- [x] Clean and transform columns (rename, retype, deduplicate)
- [x] Join and consolidate tables by key fields
- [x] Load into final destination or model-ready format

---

## ğŸ“Š Summary

ETL is the backbone of practical data science. Clean, well-prepared data leads to better models, faster iteration, and stronger business impact.

> â€œGarbage in, garbage out.â€ â€” Every Data Scientist Ever

